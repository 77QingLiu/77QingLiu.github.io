{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一. 描述一个你曾经完成或参与的数据分析的项目\n",
    "### 比如智慧供应链项目，见思维导图![思维导图](https://img.77qingliu.com/%E6%99%BA%E6%85%A7%E4%BE%9B%E5%BA%94%E9%93%BE%E9%A1%B9%E7%9B%AE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.数据集的大小，包含多少变量/字段？你曾经用过的最大数据是多少？数据的采样是如何进行的？如果是你进行采样，用了什么语言/算法/软件？\n",
    "数据集大概10亿行记录，50+原始字段。用过的最大数据是百亿行级别。通常采用随机采样方式，在Hadoop hive/Python/SAS里面实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.在数据处理中，你需要考虑哪些因素，是否需要做数据清洗？是否数据采样有取样偏差（sampling Bias）？\n",
    "##### 数据处理需要考虑的因素\n",
    "* 总体数据周期、规则、条件等的选取\n",
    "* 数据抽样规则，尤其涉及到分层、整群抽样\n",
    "* 多数据的整合、匹配和关联关系\n",
    "* 不同数据源和数据间的清洗、转换逻辑\n",
    "* 重复值、异常值和缺失值的处理逻辑\n",
    "* 数据离散化的方法选择和区间定义\n",
    "* 根据变量重要性进行数据变量的选取和降维\n",
    "\n",
    "在海量数据处理过程中，还需要考虑硬件，处理语句的性能，以及良好的编码习惯等等\n",
    "\n",
    "##### 涉及到到数据清洗主要包括：\n",
    "* 逻辑错误清洗\n",
    "    - 去重\n",
    "    - 去除不合理的值\n",
    "    - 修改矛盾的值\n",
    "\n",
    "* 缺失值清洗\n",
    "    * 确定关键字段缺失值范围\n",
    "    * 去除不需要的字段\n",
    "    * 填补缺失内容\n",
    "\n",
    "* 数据的标准化和归一化\n",
    "    * 商品字符的清洗\n",
    "    * 商品通用名统一\n",
    "    * 商品分类统一\n",
    "\n",
    "* 数据的关联性验证\n",
    "    * 多个来源数据的关联和整合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.你采用了什么样的分析/建模方法？是否这些方法符合业界标准？是否对你的数据最优？\n",
    "建模层面，对于销量预测这种回归问题，主要采用了GBDT的方法，通过手工提取时间序列到特征，采用LightGBM建模。期间也尝试了一些经典到时间序列方法，如MA，ARIMA，FaceBook Prophet，这些模型实践效果不太理想。\n",
    "上述方法在针对时间序列问题上，是比较广泛使用的方法，在kaggle上也有很多人使用。\n",
    "针对销量预测问题，我们仅仅使用了单一的，浅层的机器学习模型，如果从模型优化角度，可以从两方面进行优化\n",
    "* 一个是构造更多更好的特征\n",
    "* 另一个是可以改进模型，一方面可以尝试通过多模型融合的方式进行优化，另一方面可以尝试一些端到端的深度学习模型，如RNN，LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二. 你常用哪些方法清洗/分析数据?  \n",
    "##### 数据核验\n",
    "1. 数据格式校验，例如加0，位数，合并等，编码等。\n",
    "2. 数据逻辑的正确性，例如年龄<100等等\n",
    "\n",
    "##### 缺失值填补\n",
    "一般情况下可以通过一些简单的规则和业务逻辑进行填补，如\n",
    "1. 通过其他信息补全，例如使用身份证件号码推算性别、籍贯、出生日期、年龄等\n",
    "2. 通过前后数据补全，例如时间序列缺数据了，可以使用前后的均值\n",
    "\n",
    "也可通过建模的方法，训练算法对缺失值进行预测，例如利用数据集中其他数据的属性，可以构造一棵判定树，来预测缺失值的值。\n",
    "对于大量缺失且重要性较低对字段选择删除\n",
    "\n",
    "##### 数据的唯一性\n",
    "1. 用sql去除重复记录\n",
    "2. 按规则去重，编写一系列的规则，对重复情况复杂的数据进行去重\n",
    "\n",
    "##### 数据的权威性\n",
    "采用最权威的渠道的数据，例如药品可以通过爬虫的方式，采集食药监网站上的药品信息\n",
    "\n",
    "##### 数据的统一\n",
    "1. 与业务部门讨论确认，明确不同体系数据的统一标准。\n",
    "2. 确定规则，对数据进行统一，例如统一单位，统一名称，统一小数点位数\n",
    "\n",
    "##### 数据的转换\n",
    "1. 分类变量转换为数值变量\n",
    "2. 数值变量分段转成字符变量\n",
    "3. 数据语义转换\n",
    "4. 行列转换，表拆分等等\n",
    "\n",
    "##### 解决高维度问题\n",
    "1. 采用降维，如PCA等方式\n",
    "2. 进行特征选择\n",
    "\n",
    "##### 解决多指标数值、单位不同问题\n",
    "1. 最小-最大归一化\n",
    "2. box-cox变换\n",
    "\n",
    "##### 异常值检测和处理\n",
    "1. 逻辑检测\n",
    "    * 如判断取值范围的最大最小值进行检测与替换\n",
    "2. 简单的统计检测\n",
    "    * 如可通过Z-score进行\n",
    "    * 统计假设检验\n",
    "3. 基于模型和算法的检测\n",
    "    * 有监督的方法，如随机森林，GBDT，逻辑回归等等\n",
    "    * 无监督的方法，聚类算法如K-means，层次聚类，还有如孤立森林等算法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三. 项目测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import collections\n",
    "\n",
    "\n",
    "# Other Libraries\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report, roc_curve, average_precision_score, confusion_matrix, precision_recall_curve\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit, cross_val_score, cross_val_predict, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理和清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 原始数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/ads_train.csv\", index_col=False).drop(columns=['Unnamed: 0'])\n",
    "train.head()\n",
    "test = pd.read_csv(\"../data/ads_test.csv\", index_col=False).drop(columns=['Unnamed: 0'])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 检查目标变量的分布\n",
    "可以看到目标变量成极端不均衡分布，purchase的数据不到0.5%\n",
    "\n",
    "后续建模可能要需要resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train['y_buy'].value_counts() / train.shape[0]).to_frame('is_buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['y_buy'].value_counts()\n",
    "train['y_buy'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 检查缺失值\n",
    "仅有buy_freq这个字段在train 和 test中都存在大量缺失值，后续检查是否有合适的方法补齐数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct\n",
    "def missing_values_table(df):\n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "\n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "\n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "\n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "\n",
    "    # Sort the table by percentage of missing descending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "    mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "    '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    # Print some summary information\n",
    "    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n",
    "    \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "    \" columns that have missing values.\")\n",
    "\n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns\n",
    "missing_values = missing_values_table(train)\n",
    "missing_values.head()\n",
    "missing_values = missing_values_table(test)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 检查数据类型\n",
    "仅有几个二分类变量，其余均为数值变量，因此后续不需要针对分类变量进行特殊处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes.value_counts()\n",
    "# Number of unique classes in each categorycal column\n",
    "train.select_dtypes('int').apply(pd.Series.nunique, axis = 0).sort_values(ascending=True)\n",
    "# Only several binary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 检查连续型变量的分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['isbuyer', 'multiple_buy', 'multiple_visit', 'y_buy']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 观察到num_checkins变量取值范围较大，考虑采用normalize进行标准化\n",
    "* expected_time_buy\t和 expected_time_visit 存在负值，不明确是否是数据问题，暂时不做处理\n",
    "* last_visit 和 last_buy的分布一模一样，可能是重复字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace negtive values in uniq_urls with 0\n",
    "train['uniq_urls'] = np.where(train['uniq_urls'] < 0, 0, train['uniq_urls'])\n",
    "test['uniq_urls'] = np.where(test['uniq_urls'] < 0, 0, test['uniq_urls'])\n",
    "\n",
    "# Normalize the num_checkins features as it has extremely wide range\n",
    "train['num_checkins'], alpha = stats.boxcox(train['num_checkins'])\n",
    "test['num_checkins'] = stats.boxcox(test['num_checkins'], alpha=alpha)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补齐buy_freq缺失值\n",
    "通过数据逻辑核查，发现可以通过multiple_buy这个字段进行逻辑判断，直接填补缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "数据核验 + 缺失值填补"
   },
   "outputs": [],
   "source": [
    "# 核查有多次购买(multiple_buy), 但无2次以上历史购买的数据\n",
    "train.query(\"multiple_buy == 1 and buy_freq < 2\")\n",
    "# 核查未有多次购买(multiple_buy), 但有2次以上历史购买的数据\n",
    "train.query(\"multiple_buy == 0 and buy_freq > 1\")\n",
    "# 没有发现数据问题, 检查buy_freq缺失字段等分布\n",
    "train[train['buy_freq'].isnull()]['multiple_buy'].value_counts()\n",
    "\n",
    "# 可以看到buy_freq缺失的数据，multiple_buy都为0，因此这里通过multiple_buy这个字段，通过简单逻辑将buy_freq的缺失值填补为1\n",
    "train['buy_freq'] = np.where(train['buy_freq'].isnull(), 1, train['buy_freq'])\n",
    "test['buy_freq'] = np.where(test['buy_freq'].isnull(), 1, test['buy_freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字段含义猜测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expected_time_buy - 预期购买时间\n",
    "\n",
    "expected_time_visit - 预期浏览时间\n",
    "\n",
    "multiple_buy - 多次购买\n",
    "\n",
    "multiple_visit - 多次浏览"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 变量间互相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 9))\n",
    "\n",
    "# Heatmap of correlations\n",
    "sns.heatmap(train.corr(), cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n",
    "plt.title('Correlation Heatmap');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到last_buy 和 last_visit之间相关系数为1，这两个特征是重复的，直接删除last_visit\n",
    "* 部分变量比如购买频次和多次购买，购买间隔和多次购买之间存在强线性相关，后续线性回归可能要考虑多重共线性问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns={'last_visit'}, inplace=True)\n",
    "test.drop(columns={'last_visit'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 自变量与目标变量的相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "检测数值变量的取值范围"
   },
   "outputs": [],
   "source": [
    "# Find correlations with the target and sort\n",
    "correlations = train.corr()['y_buy'].sort_values()\n",
    "\n",
    "# Display correlations\n",
    "print('Most Positive Correlations:\\n', correlations.tail(15))\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "访问频次和多次购买和目标变量相关程度最大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "# KDE plot of loans that were repaid on time\n",
    "sns.kdeplot(train.loc[train['y_buy'] == 0, 'visit_freq'], label = 'target == 0')\n",
    "\n",
    "# KDE plot of loans which were not repaid on time\n",
    "sns.kdeplot(train.loc[train['y_buy'] == 1, 'visit_freq'], label = 'target == 1')\n",
    "# Labeling of plot\n",
    "plt.xlabel('visit frequence'); plt.ylabel('Density'); plt.title('Distribution of visit frequence');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 其他自变量和目标变量的相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = train.columns.values\n",
    "\n",
    "i = 0\n",
    "t0 = train.loc[train['y_buy'] == 0]\n",
    "t1 = train.loc[train['y_buy'] == 1]\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(4,4,figsize=(16,28))\n",
    "\n",
    "for feature in var:\n",
    "    i += 1\n",
    "    plt.subplot(4,4,i)\n",
    "    sns.kdeplot(t0[feature], bw=0.5,label=\"y_buy = 0\")\n",
    "    sns.kdeplot(t1[feature], bw=0.5,label=\"y_buy = 1\")\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过去是否有购买，是否多次购买等是目标变量等强预测指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型和算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 对原始训练数据集进行重新划分，这里采用stratified对目标变量进行分层，保证目标变量在train/test之间的分布一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sKF = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting training/testing dataset\n",
    "X = train.drop('y_buy', axis=1)\n",
    "y = train['y_buy']\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "# Turn into an array\n",
    "original_Xtrain = original_Xtrain.values\n",
    "original_Xtest = original_Xtest.values\n",
    "original_ytrain = original_ytrain.values\n",
    "original_ytest = original_ytest.values\n",
    "\n",
    "# See if both the train and test label distribution are similarly distributed\n",
    "train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n",
    "print('-' * 100)\n",
    "\n",
    "print('Label Distributions: \\n')\n",
    "print('new train: ', train_counts_label[1]/ len(original_ytrain))\n",
    "print('new test: ', test_counts_label[1]/ len(original_ytest))\n",
    "\n",
    "print('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\n",
    "print('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 未对数据进行resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 先采用简单分类模型，观测模型的效果\n",
    "* Logisitc回归\n",
    "* KNN\n",
    "* 决策树\n",
    "* 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier()\n",
    "}\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X, y)\n",
    "    training_score = cross_val_score(classifier, X, y, cv=sKF, scoring='roc_auc')\n",
    "    print(\"Classifiers: \", \n",
    "            classifier.__class__.__name__, \n",
    "          \"Has a training score of {0:.2f} auc score\".format(round(training_score.mean(), 2))\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 对模型进行超参优化调整，比较模型效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'class_weight':[None, 'balanced']}\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params, cv=sKF)\n",
    "grid_log_reg.fit(X, y)\n",
    "# We automatically get the logistic regression with the best parameters.\n",
    "log_reg = grid_log_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params, cv=sKF)\n",
    "grid_knears.fit(X, y)\n",
    "# KNears best estimator\n",
    "knears_neighbors = grid_knears.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)),\n",
    "              \"min_samples_leaf\": list(range(5,7,1))}\n",
    "grid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params, cv=sKF)\n",
    "grid_tree.fit(X, y)\n",
    "# tree best estimator\n",
    "tree_clf = grid_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in [log_reg, knears_neighbors, tree_clf]:\n",
    "    classifier.fit(X, y)\n",
    "    training_score = cross_val_score(classifier, X, y, cv=sKF, scoring='roc_auc')\n",
    "    print(\"Classifiers: \", \n",
    "            classifier.__class__.__name__, \n",
    "          \"Has a training score of {0:.2f} auc score\".format(round(training_score.mean(), 2))\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_pred = cross_val_predict(log_reg, X, y, cv=sKF,\n",
    "                             method=\"decision_function\")\n",
    "\n",
    "knears_pred = cross_val_predict(knears_neighbors, X, y, cv=sKF)\n",
    "\n",
    "tree_pred = cross_val_predict(tree_clf, X, y, cv=sKF)\n",
    "print('Logistic Regression: ', roc_auc_score(y, log_reg_pred))\n",
    "print('KNears Neighbors: ', roc_auc_score(y, knears_pred))\n",
    "print('Decision Tree Classifier: ', roc_auc_score(y, tree_pred))\n",
    "\n",
    "\n",
    "log_fpr, log_tpr, log_thresold = roc_curve(y, log_reg_pred)\n",
    "knear_fpr, knear_tpr, knear_threshold = roc_curve(y, knears_pred)\n",
    "tree_fpr, tree_tpr, tree_threshold = roc_curve(y, tree_pred)\n",
    "\n",
    "def graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, tree_fpr, tree_tpr):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('ROC Curve \\n Top 3 Classifiers', fontsize=18)\n",
    "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y, log_reg_pred)))\n",
    "    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y, knears_pred)))\n",
    "    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y, tree_pred)))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.01, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
    "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
    "                )\n",
    "    plt.legend()\n",
    "\n",
    "graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, tree_fpr, tree_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logisitic回归模型效果较好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 采用SMOTE resample方法，重新评估模型效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOTE_classify(classifier):\n",
    "    predictions = pd.Series()\n",
    "    auc = []\n",
    "    for train, test in sKF.split(X, y):\n",
    "        pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), classifier)\n",
    "        model = pipeline.fit(X.loc[train], y.loc[train])\n",
    "        prediction = pd.Series(model.predict(X.loc[test]), index=test)\n",
    "        predictions = predictions.append(prediction)\n",
    "        auc.append(roc_auc_score(y.loc[test], prediction))\n",
    "    print(\"Classifiers: \", \n",
    "                classifier.__class__.__name__, \n",
    "              \"Has a training score of {0:.2f} auc score\".format(round(np.mean(auc), 2))\n",
    "             )\n",
    "    return predictions.sort_index()\n",
    "\n",
    "log_reg_pred = SMOTE_classify(log_reg)\n",
    "knears_pred = SMOTE_classify(knears_neighbors)\n",
    "tree_pred = SMOTE_classify(tree_clf)\n",
    "\n",
    "log_fpr, log_tpr, log_thresold = roc_curve(y, log_reg_pred)\n",
    "knear_fpr, knear_tpr, knear_threshold = roc_curve(y, knears_pred)\n",
    "tree_fpr, tree_tpr, tree_threshold = roc_curve(y, tree_pred)\n",
    "graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, tree_fpr, tree_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到采用resample后模型效果有明显改善"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 基于Logisitic回归且采用SMOTE过采样的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SMOTE过采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = SMOTE().fit_resample(original_Xtrain, original_ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 变量选择\n",
    "\n",
    "这里采用LDA进行变量选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifications\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "rfecv = RFECV(estimator=lda, step=1, cv=sKF,\n",
    "              scoring='f1')\n",
    "rfecv.fit(X_resampled, y_resampled)\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()\n",
    "\n",
    "# List of selected features:\n",
    "features = [f for f,s in zip(X.columns, rfecv.support_) if s]\n",
    "print(\"Selected features: \\n{0}\".format(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_Xtrain, original_Xtest =  original_Xtrain[:,rfecv.support_], original_Xtest[:,rfecv.support_]\n",
    "X_resampled = X_resampled[:,rfecv.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logisitc回归超参数调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'class_weight':[None, 'balanced']}\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params, cv=sKF)\n",
    "grid_log_reg.fit(X_resampled, y_resampled)\n",
    "# We automatically get the logistic regression with the best parameters.\n",
    "log_reg = grid_log_reg.best_estimator_\n",
    "grid_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression model with Stratified KFold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.Series()\n",
    "auc = []\n",
    "for train, test_ in sKF.split(original_Xtrain, original_ytrain):\n",
    "    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), log_reg)\n",
    "    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n",
    "    prediction = pd.Series(model.predict(original_Xtrain[test_]), index=test_)\n",
    "    predictions = predictions.append(prediction)\n",
    "    auc.append(roc_auc_score(original_ytrain[test_], prediction))\n",
    "print(\"Classifiers: \", \n",
    "            log_reg.__class__.__name__, \n",
    "          \"Has a training score of {0:.2f} auc score\".format(round(np.mean(auc), 2))\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trian on the whole training dataset and make prediction on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_resampled, y_resampled)\n",
    "predict = log_reg.predict(original_Xtest)\n",
    "roc_auc_score(original_ytest, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 选择合适的threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        1#print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "predict_proba = log_reg.predict_proba(original_Xtest)\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "j = 1\n",
    "for i in thresholds:\n",
    "    y_test_predictions_high_recall = predict_proba[:,1] > i\n",
    "    \n",
    "    plt.subplot(3,3,j)\n",
    "    j += 1\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(original_ytest,y_test_predictions_high_recall)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    class_names = [0,1]\n",
    "    plot_confusion_matrix(cnf_matrix\n",
    "                          , classes=class_names\n",
    "                          , title='Threshold >= %s'%i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Investigate Precision-Recall curve and area under this curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal', 'red', 'yellow', 'green', 'blue','black'])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "j = 1\n",
    "for i,color in zip(thresholds,colors):\n",
    "    y_test_predictions_prob = predict_proba[:,1] > i\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(original_ytest,y_test_predictions_prob)\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    plt.plot(recall, precision, color=color,\n",
    "                 label='Threshold: %s'%i)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall')\n",
    "    plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make final prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[features]\n",
    "predict = log_reg.predict(X_test)\n",
    "predict_proba = log_reg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba[:5]"
   ]
  }
 ],
 "metadata": {
  "encoding": "# -*- coding: utf-8 -*-",
  "jupytext_format_version": "1.1",
  "jupytext_formats": "py:percent",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
